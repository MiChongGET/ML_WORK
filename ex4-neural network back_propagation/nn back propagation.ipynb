{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习4-神经网络\n",
    "通过反向传播算法实现神经网络成本函数和梯度计算的非正则化个正则化，还将实现随机权重初始化和使用网络进行预测的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "data['X'].shape,data['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也需要对我们的y标签进行一次one-hot 编码。 one-hot 编码将类标签n（k类）转换为长度为k的向量，其中索引n为“hot”（1），而其余为0。 Scikitlearn有一个内置的实用程序，我们可以使用这个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10], dtype=uint8), array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[10],y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 函数(代价函数)\n",
    "g 代表一个常用的逻辑函数（logistic function）为S形函数（Sigmoid function），公式为： \\\\[g\\left( z \\right)=\\frac{1}{1+{{e}^{-z}}}\\\\] \n",
    "合起来，我们得到逻辑回归模型的假设函数： \n",
    "\t\\\\[{{h}_{\\theta }}\\left( x \\right)=\\frac{1}{1+{{e}^{-{{\\theta }^{T}}X}}}\\\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forword Propagation(前向传播)\n",
    "> (400 + 1) -> (25 + 1) -> (10)\n",
    "\n",
    "+1是bias units （偏执单元）\n",
    "\n",
    "<img style=\"float: left;\" src=\"../img/nn_model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据上面的图可以写出下面的公式\n",
    "def forward_propagate(X,theta1,theta2):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #X的第一列插入全为1，即添加偏执单元 bias units\n",
    "    a1 = np.insert(X,0,values=np.ones(m),axis=1)\n",
    "    z2 = a1 * theta1.T \n",
    "    a2 = np.insert(sigmoid(z2),0,values=np.ones(m),axis=1)\n",
    "    z3 = a2* theta2.T\n",
    "    #获取最后的结果\n",
    "    h = sigmoid(z3)\n",
    "    \n",
    "    return a1,z2,a2,z3,h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  代价函数\n",
    "<img style=\"float: left;\" src=\"../img/nn_cost.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size 输入层\n",
    "# hidden_size 隐藏层，中间层\n",
    "def cost(params, input_size,hidden_size,num_labels,X,y,learning_rate):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size +1)],(hidden_size,(input_size + 1 ))))         \n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size +1):],(num_labels,(hidden_size + 1 ))))\n",
    "    \n",
    "    #前向传播获取各层数据\n",
    "    a1,z2,a2,z3,h = forward_propagate(X,theta1,theta2)\n",
    "    \n",
    "    #计算代价函数\n",
    "    J = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i,:],np.log(h[i,:]))\n",
    "        second_term = np.multiply((1 - y[i,:]) , np.log( 1 - h[i,:]))\n",
    "        \n",
    "        J += np.sum(first_term - second_term)\n",
    "    \n",
    "    J =  J / m\n",
    "    \n",
    "    return J    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化设置\n",
    "input_size = 400\n",
    "hidden_size = 25\n",
    "num_labels = 10 \n",
    "learningRate = 1\n",
    "\n",
    "#随机初始化完整网络参数大小的参数数组 \n",
    "params = (np.random.random(size = hidden_size *(input_size + 1 ) + num_labels *(hidden_size + 1)) -0.5) *0.25      \n",
    "\n",
    "m = X.shape[0]\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "\n",
    "#将参数数组解开为每个层的参数矩阵\n",
    "theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)],(hidden_size,(input_size +1))))       \n",
    "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):],(num_labels,(hidden_size + 1))))\n",
    "\n",
    "theta1.shape,theta2.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 25), (5000, 26), (5000, 10), (5000, 10))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算前向传播\n",
    "a1,z2,a2,z3,h = forward_propagate(X,theta1,theta2)\n",
    "a1.shape,z2.shape,a2.shape,z3.shape,h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.793965746160785"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(params,input_size,hidden_size,num_labels,X,y_onehot,learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建sigmoid函数的梯度函数\n",
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z),(1-sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 现在我们准备好实施反向传播来计算梯度。 由于反向传播所需的计算是代价函数中所需的计算过程，我们实际上将扩展代价函数以执行反向传播并返回代价和梯度。\n",
    "\n",
    "<img style=\"float: left;\" src=\"../img/神经网络代价函数.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_propagation(params ,input_size,hidden_size,num_labels,X,y,learningRateea):  \n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size +1)],(hidden_size,(input_size + 1 ))))         \n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size +1):],(num_labels,(hidden_size + 1 ))))\n",
    "    \n",
    "    #前向传播获取各层数据\n",
    "    a1,z2,a2,z3,h = forward_propagate(X,theta1,theta2)\n",
    "    \n",
    "    #初始化\n",
    "    J = 0\n",
    "    delta1 = np.zeros(theta1.shape) #(25,401)\n",
    "    delta2 = np.zeros(theta2.shape) #(10,26)\n",
    "    \n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i,:],np.log(h[i,:]))\n",
    "        second_term = np.multiply((1 - y[i,:]) , np.log( 1 - h[i,:]))\n",
    "        \n",
    "        J += np.sum(first_term - second_term)\n",
    "    \n",
    "    J =  J / m\n",
    "    #添加正则项\n",
    "    J += (float(learningRate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
    "    \n",
    "    #开始反向传播\n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:] #（1.401）\n",
    "        z2t = z2[t,:] #(1,25)\n",
    "        a2t = a2[t,:] #(1,26)\n",
    "        ht = h[t,:] #(1,10)\n",
    "        yt = y[t,:] #(1,10)\n",
    "        \n",
    "        #计算预测值和实际值的差值\n",
    "        d3t = ht - yt \n",
    "        \n",
    "        #z2添加偏执单元 bias units\n",
    "        z2t = np.insert(z2t,0,values=np.ones(1)) #添加一列值全为1，放在z2的第一列\n",
    "        d2t = np.multiply((theta2.T * d3t.T).T,sigmoid_gradient(z2t))\n",
    "        \n",
    "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
    "        delta2 = delta2 + d3t.T * a2t\n",
    "        \n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "    \n",
    "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * learningRate) / m\n",
    "    delta2[:,1:] = delta2[:,1:] +(theta2[:,1:] * learningRate) / m\n",
    "    \n",
    "    grad = np.concatenate((np.ravel(delta1),np.ravel(delta2)))\n",
    "    np.ravel(delta1)\n",
    "    \n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.799331755628472, (10285,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J,grad = background_propagation(params,input_size,hidden_size,num_labels,X,y_onehot,learningRate)           \n",
    "J,grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.3426288258922007\n",
       "     jac: array([-4.31748844e-04,  1.43958117e-06, -1.69905479e-06, ...,\n",
       "        4.62757165e-04,  2.27110302e-04,  4.55018223e-05])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 19\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 0.03088616,  0.00719791, -0.00849527, ...,  0.17615089,\n",
       "        1.96005331, -0.39659801])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "fmin  = minimize(fun=background_propagation,\n",
    "                 x0=params,\n",
    "                 args=(input_size, hidden_size, num_labels, X, y_onehot, learningRate),\n",
    "                 method='TNC',jac=True,options={'maxiter':250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.matrix(X)\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size +1)],(hidden_size,(input_size + 1 ))))         \n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size +1):],(num_labels,(hidden_size + 1 ))))\n",
    "\n",
    "a1 ,z2 ,a2 , z3 ,h = forward_propagate(X,theta1,theta2)\n",
    "y_pred = np.array(np.argmax(h,axis=1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.14%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a,b) in zip(y_pred,y)]\n",
    "accuracy = (sum(map(int,correct)) / float(len(correct)))\n",
    "print('accuracy = {0}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
